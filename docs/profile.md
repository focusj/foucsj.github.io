## 个人信息

- 姓名：王康；性别：男；生日：1989/12/24；婚否：已婚；
- 电话：18610842638； Email：focusj.x@gmail.com；
- 毕业院校：河北大学工商学院（本科）；毕业时间：2012/7/01；专业：信息管理与信息系统；
- 博客：http://www.jianshu.com/users/13542edebea3/

## 工作经历

### 1. 2020年7月~今：区块链钱包
工作职责：
- 负责钱包Dex业务系统后端工作，日常的开发维护，系统优化，架构升级等；
- 与合约组协作，对接新功能；

### 2. 2019年3月~2020年7月：字节跳动
工作职责：
- 系统架构优化升级;
- 系统服务化重构改造；

### 3. 2017年11月~2019年3月：北京摩拜信息技术有限公司
工作职责：
- 负责从零搭建公司监控系统，包括：系统架构设计，子系统开发及项目进度把控。
- 推进监控系统在公司业务部门的对接和使用。

### 4. 2016年03月~2017年11月：北京第三石信息科技有限公司
工作职责：
- 后端核心开发，主要负责后端系统的设计和开发。
- 系统重构和微服务架构改造。

### 5. 2014年10月~2015年11月：ThoughtWorks
工作职责：
- 业务系统全栈开发

### 6. 2012年04月~2014年09月：北京尤尼信息科技有限公司
工作职责：
- 负责公司后端系统的开发和维护。
- 兼职项目经理，负责和产品经理对接和梳理需求，制定迭代计划。 

## 技术总结

- 熟悉单体、微服务，事件驱动，CQRS，Serverless等多种架构模型，善于分析建模复杂业务，并选择合适的架构；
- Clean Coder，擅于老系统的重构与优化工作，Problem Solver；
- 熟悉常见编程语言Golang，Java，Python，Scala等多种面向对象、函数式编程语言；
- 熟悉并发编程和异步编程，擅于分析性能瓶颈并优化。
- 熟悉缓存，存储，消息队列等中间件的使用（MongoDB，MySQL，InfluxDB, Redis，Kafka等）。
- 了解SRE工程实践，对系统监控报警，Tracing等都较为熟悉。
- 了解DevOps，服务网格，熟悉Docker，Kubernetes，istio等工具的使用。
- 熟悉敏捷项目管理，敏捷开发，极限编程，对CI/CD流程较为熟悉。
- Web3，了解以太坊生态，对钱包，Dex等业务有一定了解；

## 项目经验
### Mev Bot套利项目
- 项目描述：基于以太坊的MEV套利项目
- 技术栈：ETH，Go，ants，LevelDB，Neo4j，Solidity

#### Mev Bot后端架构设计
Mev Bot后端系统主要是用Go语言实现，设计的目标是构建一个Mev Event的流式处理管道，高效的处理流入系统内部Mev Event。系统主要分为四个模块：

- Event Collector：负责接收收集外部Mev event（mev share，pending tx，new block等），另外这个模块还兼顾event decode的功能，将event中携带的amm pool信息decode出来，送入pipeline；
- Pool Finder：以pool为搜索条件，搜索所有与该pool相关的Mev套利机会。该模块还设计一些策略：利润模拟，套利机会组合，用于提升套利机会的成功率；
- Sender：根据套利机会构建套利Transaction，并将这些tx提交到不同的Mev share service（或builder）；
- Indexer：从链上获取所有主要AMM协议的流动性资料（pair，token等），并存入LevelDB，供pool finder模块使用；

#### 技术难点
- 快速处理每个套利event（个位数毫秒内），，这是确保数据管道能够高效运行的基础。解决方案：
    - 提前索引链上的pool，token信息，并加以缓存，finder在搜索套利机会时几乎都会命中缓存；
    - 大量使用goroutine来并发的处理历程中耗时的工作，可以异步处理的环节也尽可能的异步处理；为了更好的管理goroutine这里我们使用ants来管理goroutine和异步任务；
- 最大限度使用免费的节点服务，克服单个节点对qps和使用总量限制。解决方案：
    - 重新封装eth client，使之能够支持多个rpc endpoint。
    - 在发送请求的时候使用round-robbin策略来分散请求量，跳过单个rpc的QPS限制，分散用量；
    - 借鉴断路器（circuit breaker）模式管理每个rpc的状态，确保所有的请求都能够使用健康状态的rpc；
- 灵活控制builder的请求数量，每个builder对send bundle都有qps限制，发送太快太多会进黑名单。解决方案：
    - 这里借鉴rate limit思路，来管理builder被调用的频率。
    - 再配合ants task pool实现在安全的访问速率内最快的发送所有的bundle；
- 多边形套利机会搜索速度优化，由于硬件资源受限我们引入的Neo4j数据库的查询普遍都在几十秒，并且系统负载极高。解决方案：
    - 首先限制数据库的并发访问，将数据库维持在一个健康的负载，抽象DB Accessor，内部维护n个携程去访问数据库，并增加队列处理排队的请求；
    - 封装基于LevelDB持久化的缓存，应对应用重启缓存重建；
    - 优化业务层逻辑：每次搜索路径时，优先查询缓存，如果命中则直接返回；如果没有则提交搜索任务到DB Accessor；这时查询线程会再重试3次缓存查询，以便期间查询任务成功更新缓存能拿到数据；

### Dex 后端系统
- 项目描述：主要负责维护dex的后端系统，负责系统重构优化，架构升级，日常开发和运维；
- 技术栈：Go，Java，MySQL，Kafka；

#### Dex后端系统V5架构升级
- Dex后端系统一共有三条数据链路：报价链路，提交订单，订单状态更新。我们此次升级要考虑的重点是最大限度兼容老版本。所以整体的设计思路是重构三条数据链路，兼容升级前后业务模型的差异。
- 此次升级我们做的另外一个重要的事情是，对原有系统一些不合理的地方进行重构和改造。原来系统最大的问题是，盲目追求消息异步和过度拆分微服务。这导致系统的调用链路长且脆弱，过量的微服务也带来了不必要的稳定性问题和运维负担。因此在这次升级中我们也对消息链路进行了切割，对功能类似的微服务进行来了删减和合并。
- 交易挖矿奖励：按照算力分配规则计算每个角色在当下结算周期中应得的算力积分。这里的难点：因为我们做的是链上系统，所有的数据都是可追溯的。所以算出来的任何数据都要经得起审查，确保数据的绝对正确是前提。所以，在设计系统的时候所有的计算步骤都可以设计成幂等的，能够重复的执行计算步骤。

### multi-hop路径搜索算法
原始的Dex服务对接的是专业做市商，做市商能够支持什么流动性，我们才能支持到这些交易，交易对数量非常有限。这个算法的主要功能在于基于主流的这些AMM协议，借助一些稳定币ETH/USDT/USDC/DAI/WBTC作为桥接的币种，去组合所有可能的交易币种；

#### 算法细节
- 搜索0 hop，即可直接交易的币对：将所有的原始流动性(Token0, Token1)放入Set数据结构: PoolsSet中（可以先把Token0，Token1进行排序），传入目标Token0/Token1，直接查询Set；
- 搜索1 hop，即通过1个桥接Token可交易：
    - 先根据稳定币列表构建潜在的交易路径：baseToken0s=[(Token0, BaseToken)] & baseToken1s=[(BaseToken, Token1)];
    - 筛选出baseToken0s中存在流动性的Pool，baseToken0s与PoolsSet的交集；
    - 筛选出baseToken1s中存在流动性的Pool，baseToken1s与PoolsSet的交集；
    - 第2,3步存在交集的BaseToken，即为可成功桥接的Tokens；
- 搜索2 hop，即通过2个桥接Token可交易：
    - 先根据稳定币列表构建潜在的交易路径：baseToken0s=[(Token0, BaseToken)] & baseToken1s=[(BaseToken, Token1)];
    - 筛选出baseToken0s中存在流动性的Pool，baseToken0s与PoolsSet的交集；
    - 筛选出baseToken1s中存在流动性的Pool，baseToken1s与PoolsSet的交集；
    - 构建潜在的稳定币Pool，bridgePools=[(base0, base1)]，并筛选出bridgePools中存在流动性的Pool；
    - 排列第2,3步的结果，从中拿出两个base token，检查它是否存在bridge中；


### 字节跳动审核平台架构优化
*项目信息*

项目描述：负责字节跳动公司内审核平台的存储层重构，架构升级，以及服务化改造工作。

技术栈：Go，Python，Thrift，Kafka，MongoDB，Abase（KV），ES。

周期：2019/09 ~ 2020/06

*项目职责*

#### 实现DRC数据同步组件

实现DRC数据同步组件，服务异构存储间数据同步。整个DRC分为三个模块：oplog收集模块负责从MongoDB拉取增量日志推送到kafka；数据拼装模块消费kafka并从db查询一次最新数据转送另一topic；sink模块消费拼装好的消息，将数据写入ES，Abase，HDFS等不同的数据源。

数据拼装模块是非常典型的生产者消费者模型。实现层面生产者占用主进程，负责从kafka拉取多条消息，下发到不同的worker中。每个Worker是独立的进程，同时每个worker中又有多个线程并发处理消息。这种多进程+多线程模型最大限度提升了消费能力（单纯用Python的进程和线程都会存在问题）。worker在接收到消息时也会作一遍去重操作，减少DB的查询次数。

在数据一致性上做到了程序层面的弱一致性保障：1. 尽可能保证消息在消费过程中不丢；2. 尽可能保证消息不乱。

项目收益：
- 数据同步实时性从原来几十秒提升至ptc99延时750ms；
- 数据一致性从原来的2个9提升至7个9+；
- 性能大幅提升，每月节省物理资源费用5000/月。

#### 存储层优化

存储层优化，提升存储层稳定性和性能，构建统一数据服务。该项目主要为了解决平台在存储层面临的问题：1. 数据分区策略缺陷，导致热门业务数据分区过大，影响mongo稳定性；2. 由于业务特性数据周期生命周期一般小于1月，大量的过期数据严重影响了mongo的性能和稳定性；3. 平台内部引入了多套存储系统（ES，KV），但均未充分发挥作用，读写主要还集中在mongo。
   
基于上述痛点，主要提出以下改进方案：1. 冷热数据隔离，将处于业务完成态的数据迁移到KV系统，只在mongo中保存热数据；2. 充分发挥es的检索优势和kv的性能有事构造查询引擎，以减轻mongo的压力；3. 抽象数据服务，隔离所有的数据操作服务，提供统一数据出口。项目收益：

- 冷热数据分离，减少MongoDB数据量2/5。消除了数据库慢查，数据库整体稳定性大幅提升。同时为后续数据分区优化奠定基础；
- 充分利用es和kv的各自优势，构建查询引擎，查询接口延时平均下降20%。
- 构建数据中层服务，统一数据出口，同时，推动后续服务改造和数据中台建设。
- 将数据库原生oplog数据对外开发，提升其他系统使用平台数据的灵活性。

#### 任务引擎重构

当前审核任务的实现方式是通过脚本拼凑来实现的，项目里到处都有处理任务状态流转的代码。导致的问题是，经常有任务状态流转出出错，任务跑到异常状态，影响审核员工作。这块新方案计划引入状态机来集中管理任务状态流转，预期达到效果：
- 保证任务状态流转的完备性，不会出现错误状态任务；
- 增加程序的健壮性，通过持久化Event的方式实现状态流转可重试，可回滚，可追溯；
- Event是和业务操作一一对应的，通过Flink消费Event流，计算目前系统上的实时指标。

#### SRE和效能实践

1. 在项目内引入持续集成实践，并在组内推广使用。同时培训项目成员如何写UT和TDD开发。
2. 梳理监控指标体系，构建系统SLA。从稳定性（服务在线时间）和服务质量（正确率和延时）两个维度梳理指标，并对指标进行埋点，构建监控看板。

### 审核平台GDPR改造

*项目信息*

项目描述：负责推进平台的GDPR改造，并推动业务方进行服务迁移。

技术栈：Python，Django，MongoDB

周期：2019/05~2019/08

*项目职责*

1. 完成系统技术改造，配置和生产代码分离，实现系统单元化部署能力；
2. 实现数据迁移系统，确保各业务线安全平稳安全迁移，最终实现上万队列的迁移；
3. 作为项目owner整体把控项目进度，确保项目如期交付；

### 摩拜监控系统

*项目信息*

项目描述：从零开始搭建摩拜内部监控系统，目前所有的核心服务都已接入监控服务。每天监控系统处理7亿+指标信息(100G)，发出有效报警上百条。

技术栈：Java，Go，Spring Cloud，Istio，Kubernetes，gRPC，InfluxDB，ES

周期：2018/02 ~ 2019/03

*项目职责*

1. 从零搭建公司业务监控系统，协助所有核心业务系统均已接入监控服务，现在每天处理7亿+指标量。
2. 使用Golang开发基于内存的实时报警策略计算模块，报警策略表达式可以支持配置阈值和同环比，同时还提供了降噪功能，防止瞬间抖动造成误报。实时性方面：一个异常指标从应用端上报到报警发出整个链路耗时可控制在1min之内。
3. 基于influx-proxy的做了InfluxDB双区多实例高可用方案。在原有开源项目上开发了动态更新数据节点配置的功能。
4. 优化数据消费服务指标判重逻辑。原有的实现方式是：直接查询MySQL数据库，程序使用线程池提升处理能力。但由于数据量太大导致了线程池队列积压，引发了OOM。通过引入布隆过滤器重新优化了这部分实现，不仅解决了原有问题还加快了系统处理速度。
5. 基于Kubernetes和Istio的ServiceMesh改造，并且基于Kubernetes和Istio API实现了灰度发布工具。
6. 使用Golang和Mux开发了内部短域名服务，上线半月的时间已经生成了7000多万个code。

*技术栈*：SpringCloud，Java8，Golang，gRPC，Istio，Kubernetes，InfluxDB，ElasticSearch，Grafana，Telegraf。

### 第三石订单系统

*项目信息*

项目描述：北京第三石在美国市场做C2C业务，主要是线下二手物品交易。为了让用户交易更安全方便，公司要做担保交易和物流。

技术栈： Java 8，Spring Boot，Spring StateMachine，JOOQ，Akka，Flyway，RabbitMQ，Redis，MySQL

周期：2016/09 ~ 2017/01

*项目职责*：

1. 引入Akka框架实现异步编程（将业务流程中的非关键流程异步话，比如发送push，订单扣费邮件等都可异步化），缩短服务的响应时间：订单接口从2~3s缩短到1s内。
2. 引入Spring StateMachine重构订单状态流转逻辑，增强了程序的可维护性。并通过其提供的回调机制实现订单历史状态持久化。
3. 基于Redlock算法实现了Java版本的Redis分布式锁。
4. 按照REST风格设计API，并通过swagger提供在线文档，降低前后端对接的难度。


### 第三石微服务重构

*项目信息*

项目描述：北京第三石是成立了三年多的创业公司，由于缺乏对后端服务重构和优化，所有的业务都耦合在一个系统里，导致系统臃肿复杂。开发维护成本越来越高。而且Python + Django也导致了严重的性能问题。为了解决这些问题，开始推进后端服务的拆分，以及技术栈转型.

技术栈：Java8，Vert.x，JOOQ，MySQL，Redis，Docker，etcd。

周期：2017/03 ~ 2018/10

*项目职责*

1. 系统边界划分以及微服务拆分规划。通过对现在业务的盘点和梳理，一共抽象了5个基础服务：用户，商品，订单，物流，聊天。实施方案如下：将现有系统中和基础服务相关的数据操作代码提取出来，包装为服务；接下来将剩余代码，按照不同的业务归属拆分为不同服务。
2. 技术调研和技术选型。用户、商品这些基础服务对性能要求比较高，综合对比了SpringBoot和Vertx，最终选择Vert.x作为基础服务框架。
3. 结合Redis缓存，最终用户服务完成后性能测试（4G/4Core）：读7000+QPS，写3000+QPS。
   
### 名称：业务系统

*项目信息*

项目描述：该项目是ThoughtWorks为某美国公司开发的业务处理系统。该公司主要负责为第三方公司提供财务审计，国际报税等业务。每项业务都由单独的系统处理。为了方便第三方客户查看及使用，需要作一个聚合的网站，把现有的业务聚合到一个网站内操作.

技术栈：.Net Web API，ReactJS，SQL Server

周期：2014/10 ~ 2015/11

*项目经历*：

- 引入Pact契约测试框架，提前暴露由接口协议改变而引发的微服务集成失败。
- 引入React封装前端通用组件，减少团队的重复性工作，保证网站体验的一致性。
- 丰富的敏捷开发实战体验：TDD，CodeReview，CI，CD等。

